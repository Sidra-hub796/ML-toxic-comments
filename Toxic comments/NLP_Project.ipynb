{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy (if not already installed)\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas and scikit-learn for data handling and evaluation\n",
    "#!pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "from spacy.pipeline.textcat import Config, single_label_cnn_config, single_label_bow_config, single_label_default_config\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")  # Tokenization\n",
    "nltk.download(\"stopwords\")  # Stop words\n",
    "nltk.download(\"vader_lexicon\")  # Sentiment analysis\n",
    "nltk.download(\"wordnet\")  # Lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train.csv\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and overall dataset structure\n",
    "print(\"Missing values in the dataset:\")\n",
    "print(train_data.isnull().sum())\n",
    "train_data.info()\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'severe_toxic' class because of redunancy with toxic\n",
    "train_data = train_data.drop(columns=['severe_toxic'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs and outputs\n",
    "X = train_data[\"comment_text\"]\n",
    "y = train_data.drop(columns=[\"id\", \"comment_text\"])\n",
    "\n",
    "# Inspect labels for imbalance\n",
    "print(\"Label distribution:\")\n",
    "print(y.sum(axis=0))\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words and spacy model\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):        \n",
    "    text = text.lower() # Convert to lowercase    \n",
    "    \n",
    "    text = re.sub(r\"\\s+\", \" \", text) # Remove extra whitespaces    \n",
    "    \n",
    "    text = re.sub(r\"http\\S+\", \"\", text) # Remove URLs    \n",
    "    \n",
    "    text = re.sub(r\"[^\\w\\s!?]\", \"\", text) # Remove punctuation, but keep \"!\" and \"?\" for context    \n",
    "    \n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text) # Normalize repeated characters (\"soooo\" -> \"soo\")    \n",
    "    \n",
    "    tokens = word_tokenize(text) # Tokenize text    \n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stop_words] # Remove stopwords    \n",
    "    \n",
    "    tokens = [token.lemma_ for token in nlp(\" \".join(tokens))] # Lemmatize tokens    \n",
    "    \n",
    "    return \" \".join(tokens) # Reconstruct text from processed tokens\n",
    "\n",
    "# Function to transform the data into spacy format\n",
    "def prepare_spacy_data(comments, labels):    \n",
    "    spacy_data = []\n",
    "    for comment, label in zip(comments, labels.values):\n",
    "        cats = {col: bool(value) for col, value in zip(labels.columns, label)}  # Convert labels to binary dict\n",
    "        spacy_data.append((comment, {\"cats\": cats}))\n",
    "    return spacy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "X_train_cleaned = X_train.apply(preprocess_text)\n",
    "X_val_cleaned = X_val.apply(preprocess_text)\n",
    "\n",
    "# Print sample of preprocessed text\n",
    "print(\"Sample cleaned comments:\")\n",
    "print(X_train_cleaned[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform preprocessed data into spaCy format\n",
    "train_data_spacy = prepare_spacy_data(X_train_cleaned, y_train)\n",
    "val_data_spacy = prepare_spacy_data(X_val_cleaned, y_val)\n",
    "\n",
    "# Print sample of processed spaCy data\n",
    "print(\"Sample processed data (spaCy format):\")\n",
    "print(train_data_spacy[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to files (optional)\n",
    "# import json\n",
    "\n",
    "# with open(\"train_data_spacy.json\", \"w\") as f:\n",
    "#     json.dump(train_data_spacy, f)\n",
    "\n",
    "# with open(\"val_data_spacy.json\", \"w\") as f:\n",
    "#     json.dump(val_data_spacy, f)\n",
    "\n",
    "# print(\"Preprocessed data saved as JSON files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_distribution = {\n",
    "    \"toxic\": 15294,\n",
    "    \"insult\": 7877,\n",
    "    \"obscene\": 8449,\n",
    "    \"identity_hate\": 1405,\n",
    "    \"threat\": 478\n",
    "}\n",
    "total = sum(label_distribution.values())\n",
    "class_weights = {label: total / freq for label, freq in label_distribution.items()}\n",
    "\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.example import Example\n",
    "from thinc.types import Floats2d\n",
    "\n",
    "def custom_loss_function(weights):\n",
    "    def loss(optimizer, examples, predictions):\n",
    "        total_loss = 0.0\n",
    "        for example, prediction in zip(examples, predictions):\n",
    "            true_cats = example.reference.cats\n",
    "            pred_cats = prediction.get(\"cats\", {})\n",
    "\n",
    "            for label, true_value in true_cats.items():\n",
    "                pred_value = pred_cats.get(label, 0.0)\n",
    "                weight = weights.get(label, 1.0)  # Default weight is 1.0\n",
    "                # Weighted binary cross-entropy loss\n",
    "                loss = -weight * (\n",
    "                    true_value * np.log(pred_value + 1e-8) +  # Avoid log(0)\n",
    "                    (1 - true_value) * np.log(1 - pred_value + 1e-8)\n",
    "                )\n",
    "                total_loss += loss\n",
    "\n",
    "        # Backpropagate the total loss\n",
    "        optimizer.backward(Floats2d([total_loss]))\n",
    "        return total_loss\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.types import Floats2d\n",
    "import numpy as np\n",
    "\n",
    "# Custom loss function for multi-label classification\n",
    "def custom_loss(predictions, labels, weights):\n",
    "    total_loss = 0.0\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for cat, true_value in label.items():\n",
    "            pred_value = prediction.get(cat, 0.0)\n",
    "            weight = weights.get(cat, 1.0)\n",
    "            # Weighted binary cross-entropy loss\n",
    "            loss = -weight * (\n",
    "                true_value * np.log(pred_value + 1e-8) +\n",
    "                (1 - true_value) * np.log(1 - pred_value + 1e-8)\n",
    "            )\n",
    "            total_loss += loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank spaCy pipeline for English\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Print a confirmation\n",
    "print(\"Blank spaCy pipeline created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"threshold\": 0.5,  # Classification threshold\n",
    "    \"model\": {\n",
    "        \"@architectures\": \"spacy.TextCatEnsemble.v2\",\n",
    "        \"tok2vec\": {\n",
    "            \"@architectures\": \"spacy.Tok2Vec.v2\",\n",
    "            \"embed\": {\n",
    "                \"@architectures\": \"spacy.MultiHashEmbed.v2\",\n",
    "                \"width\": 64,\n",
    "                \"rows\": [2000, 2000, 500, 1000, 500],\n",
    "                \"attrs\": [\"NORM\", \"LOWER\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"],\n",
    "                \"include_static_vectors\": False,\n",
    "            },\n",
    "            \"encode\": {\n",
    "                \"@architectures\": \"spacy.MaxoutWindowEncoder.v2\",\n",
    "                \"width\": 64,\n",
    "                \"window_size\": 1,\n",
    "                \"maxout_pieces\": 3,\n",
    "                \"depth\": 2,\n",
    "            },\n",
    "        },\n",
    "        \"linear_model\": {\n",
    "            \"@architectures\": \"spacy.TextCatBOW.v3\",\n",
    "            \"exclusive_classes\": False,  # Multi-label classification\n",
    "            \"ngram_size\": 1,\n",
    "            \"no_output_layer\": False,\n",
    "            \"length\": 262144,  # Add length explicitly to avoid further errors\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add the multi-label text categorizer\n",
    "# textcat = nlp.add_pipe(\"textcat_multilabel\", config=config)\n",
    "\n",
    "# Add labels (categories) to the text categorizer\n",
    "for label in y_train.columns:  # Assuming y_train.columns contains category names\n",
    "    textcat.add_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Create the weighted loss function\n",
    "loss_function = custom_loss_function(class_weights)\n",
    "\n",
    "# Training parameters\n",
    "n_iter = 1  # Number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_iter):\n",
    "    random.shuffle(train_data_spacy)  # Shuffle training data each epoch\n",
    "    losses = {}\n",
    "\n",
    "    # Create batches of data\n",
    "    batches = minibatch(train_data_spacy, size=compounding(4.0, 32.0, 1.001))\n",
    "\n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        labels = []  # True labels for custom loss\n",
    "        predictions = []  # Predicted scores for custom loss\n",
    "\n",
    "        for text, annotations in batch:\n",
    "            # Create Example objects\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, {\"cats\": annotations[\"cats\"]})  # Multi-label format\n",
    "            examples.append(example)\n",
    "            labels.append(annotations[\"cats\"])  # Collect true labels\n",
    "\n",
    "        # Update the model and collect predictions\n",
    "        nlp.update(\n",
    "            examples,\n",
    "            drop=0.5,\n",
    "            losses=losses,\n",
    "            sgd=optimizer\n",
    "        )\n",
    "\n",
    "        # Collect predictions from the model\n",
    "        for example in examples:\n",
    "            predictions.append(example.predicted.cats)\n",
    "\n",
    "        # Compute custom loss\n",
    "        total_custom_loss = custom_loss(predictions, labels, class_weights)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss (SpaCy): {losses['textcat_multilabel']}, Custom Loss: {total_custom_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions and true labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for text, annotations in val_data_spacy:  # val_data is a list of (text, annotations)\n",
    "    # Convert the text and annotations into an Example\n",
    "    example = spacy.training.Example.from_dict(nlp.make_doc(text), {\"cats\": annotations[\"cats\"]})\n",
    "    \n",
    "    # Process the text with the model\n",
    "    doc = nlp(example.text)\n",
    "    \n",
    "    # Collect the predictions\n",
    "    pred_labels.append({label: doc.cats[label] for label in doc.cats})\n",
    "    \n",
    "    # Collect the true labels\n",
    "    true_labels.append(annotations[\"cats\"])\n",
    "\n",
    "# Convert predictions to binary based on threshold\n",
    "threshold = 0.5\n",
    "pred_binary = [\n",
    "    {label: int(score >= threshold) for label, score in pred.items()}\n",
    "    for pred in pred_labels\n",
    "]\n",
    "\n",
    "# Extract the keys (categories/labels) that are present in all true labels\n",
    "categories = list(set.intersection(*[set(label_dict.keys()) for label_dict in true_labels])) # Made categories only include labels present in all true_labels to avoid KeyError\n",
    "\n",
    "# Convert dictionaries to 2D arrays for sklearn\n",
    "true_array = np.array([[label_dict.get(cat, 0) for cat in categories] for label_dict in true_labels])\n",
    "pred_array = np.array([[label_dict.get(cat, 0) for cat in categories] for label_dict in pred_binary])\n",
    "\n",
    "# Evaluate using sklearn's classification report\n",
    "print(classification_report(true_array, pred_array, target_names=categories))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
