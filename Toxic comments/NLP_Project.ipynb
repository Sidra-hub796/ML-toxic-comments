{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy (if not already installed)\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas and scikit-learn for data handling and evaluation\n",
    "#!pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"punkt\")  # Tokenization\n",
    "nltk.download(\"stopwords\")  # Stop words\n",
    "nltk.download(\"vader_lexicon\")  # Sentiment analysis\n",
    "nltk.download(\"wordnet\")  # Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "from spacy.pipeline.textcat import Config, single_label_cnn_config, single_label_bow_config, single_label_default_config\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train.csv\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and overall dataset structure\n",
    "print(\"Missing values in the dataset:\")\n",
    "print(train_data.isnull().sum())\n",
    "train_data.info()\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disclaimerhiscelltakes22minutestorunonmymachine# Load stop words and spacy model\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text data.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove punctuation, but keep \"!\" and \"?\" for context\n",
    "    text = re.sub(r\"[^\\w\\s!?]\", \"\", text)\n",
    "    \n",
    "    # Normalize repeated characters (\"soooo\" -> \"soo\")\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    \n",
    "    # Reconstruct text from processed tokens\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Function to transform the data into spacy format\n",
    "def prepare_spacy_data(comments, labels):\n",
    "    \"\"\"Transform text and labels into spacy-compatible format.\"\"\"\n",
    "    spacy_data = []\n",
    "    for comment, label in zip(comments, labels.values):\n",
    "        cats = {col: bool(value) for col, value in zip(labels.columns, label)}  # Convert labels to binary dict\n",
    "        spacy_data.append((comment, {\"cats\": cats}))\n",
    "    return spacy_data\n",
    "\n",
    "# Define inputs and outputs\n",
    "X = train_data[\"comment_text\"]\n",
    "y = train_data.drop(columns=[\"id\", \"comment_text\"])\n",
    "\n",
    "# Preprocess text data\n",
    "X_cleaned = X.apply(preprocess_text)\n",
    "\n",
    "# Print sample of preprocessed text\n",
    "print(\"Sample cleaned comments:\")\n",
    "print(X_cleaned[:3])\n",
    "\n",
    "# Inspect labels for imbalance\n",
    "print(\"Label distribution:\")\n",
    "print(y.sum(axis=0))\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_cleaned, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# Transform preprocessed data into spaCy format\n",
    "train_data_spacy = prepare_spacy_data(X_train, y_train)\n",
    "val_data_spacy = prepare_spacy_data(X_val, y_val)\n",
    "\n",
    "# Print sample of processed spaCy data\n",
    "print(\"Sample processed data (spaCy format):\")\n",
    "print(train_data_spacy[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'severe_toxic' class because of redunancy with toxic\n",
    "train_data = train_data.drop(columns=['severe_toxic'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input (comments) and output (labels)\n",
    "X = train_data['comment_text']\n",
    "y = train_data.drop(columns=['id', 'comment_text'])\n",
    "\n",
    "# Inspect labels for imbalance\n",
    "print(\"Label distribution:\")\n",
    "print(y.sum(axis=0))  # Check class counts per label\n",
    "\n",
    "# Split the data (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform the data into spaCy format\n",
    "def prepare_spacy_data(comments, labels):\n",
    "    spacy_data = []\n",
    "    for comment, label in zip(comments, labels.values):\n",
    "        # Create a dictionary of labels with their binary values\n",
    "        cats = {col: bool(value) for col, value in zip(labels.columns, label)}\n",
    "        spacy_data.append((comment, {'cats': cats}))\n",
    "    return spacy_data\n",
    "\n",
    "# Prepare training and validation data\n",
    "train_data_spacy = prepare_spacy_data(X_train, y_train)\n",
    "val_data_spacy = prepare_spacy_data(X_val, y_val)\n",
    "\n",
    "# Print a sample of the processed data\n",
    "print(\"Sample processed data (spaCy format):\")\n",
    "print(train_data_spacy[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespaces\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to training and validation data\n",
    "X_train_cleaned = X_train.apply(preprocess_text)\n",
    "X_val_cleaned = X_val.apply(preprocess_text)\n",
    "\n",
    "# Prepare spaCy data again with cleaned text\n",
    "train_data_spacy = prepare_spacy_data(X_train_cleaned, y_train)\n",
    "val_data_spacy = prepare_spacy_data(X_val_cleaned, y_val)\n",
    "\n",
    "print(\"Sample cleaned data (spaCy format):\")\n",
    "print(train_data_spacy[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to files (optional)\n",
    "# import json\n",
    "\n",
    "# with open(\"train_data_spacy.json\", \"w\") as f:\n",
    "#     json.dump(train_data_spacy, f)\n",
    "\n",
    "# with open(\"val_data_spacy.json\", \"w\") as f:\n",
    "#     json.dump(val_data_spacy, f)\n",
    "\n",
    "# print(\"Preprocessed data saved as JSON files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank spaCy pipeline for English\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Print a confirmation\n",
    "print(\"Blank spaCy pipeline created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"threshold\": 0.5,  # Classification threshold\n",
    "    \"model\": {\n",
    "        \"@architectures\": \"spacy.TextCatEnsemble.v2\",\n",
    "        \"tok2vec\": {\n",
    "            \"@architectures\": \"spacy.Tok2Vec.v2\",\n",
    "            \"embed\": {\n",
    "                \"@architectures\": \"spacy.MultiHashEmbed.v2\",\n",
    "                \"width\": 64,\n",
    "                \"rows\": [2000, 2000, 500, 1000, 500],\n",
    "                \"attrs\": [\"NORM\", \"LOWER\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"],\n",
    "                \"include_static_vectors\": False,\n",
    "            },\n",
    "            \"encode\": {\n",
    "                \"@architectures\": \"spacy.MaxoutWindowEncoder.v2\",\n",
    "                \"width\": 64,\n",
    "                \"window_size\": 1,\n",
    "                \"maxout_pieces\": 3,\n",
    "                \"depth\": 2,\n",
    "            },\n",
    "        },\n",
    "        \"linear_model\": {\n",
    "            \"@architectures\": \"spacy.TextCatBOW.v3\",\n",
    "            \"exclusive_classes\": False,  # Multi-label classification\n",
    "            \"ngram_size\": 1,\n",
    "            \"no_output_layer\": False,\n",
    "            \"length\": 262144,  # Add length explicitly to avoid further errors\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add the multi-label text categorizer\n",
    "textcat = nlp.add_pipe(\"textcat_multilabel\", config=config)\n",
    "\n",
    "# Add labels (categories) to the text categorizer\n",
    "for label in y_train.columns:  # Assuming y_train.columns contains category names\n",
    "    textcat.add_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Training parameters\n",
    "n_iter = 1  # Number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_iter):\n",
    "    random.shuffle(train_data_spacy)  # Shuffle training data each epoch\n",
    "    losses = {}\n",
    "    \n",
    "    # Create batches of data\n",
    "    batches = minibatch(train_data_spacy, size=compounding(4.0, 32.0, 1.001))\n",
    "    \n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, annotations in batch:\n",
    "            # Create Example objects\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, {\"cats\": annotations[\"cats\"]})  # Multi-label format\n",
    "            examples.append(example)\n",
    "        \n",
    "        # Update the model with the batch of Example objects\n",
    "        nlp.update(examples, drop=0.5, losses=losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {losses['textcat_multilabel']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions and true labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for text, annotations in val_data_spacy:  # val_data is a list of (text, annotations)\n",
    "    # Convert the text and annotations into an Example\n",
    "    example = spacy.training.Example.from_dict(nlp.make_doc(text), {\"cats\": annotations[\"cats\"]})\n",
    "    \n",
    "    # Process the text with the model\n",
    "    doc = nlp(example.text)\n",
    "    \n",
    "    # Collect the predictions\n",
    "    pred_labels.append({label: doc.cats[label] for label in doc.cats})\n",
    "    \n",
    "    # Collect the true labels\n",
    "    true_labels.append(annotations[\"cats\"])\n",
    "\n",
    "# Convert predictions to binary based on threshold\n",
    "threshold = 0.5\n",
    "pred_binary = [\n",
    "    {label: int(score >= threshold) for label, score in pred.items()}\n",
    "    for pred in pred_labels\n",
    "]\n",
    "\n",
    "# Extract the keys (categories/labels) that are present in all true labels\n",
    "categories = list(set.intersection(*[set(label_dict.keys()) for label_dict in true_labels])) # Made categories only include labels present in all true_labels to avoid KeyError\n",
    "\n",
    "# Convert dictionaries to 2D arrays for sklearn\n",
    "true_array = np.array([[label_dict.get(cat, 0) for cat in categories] for label_dict in true_labels])\n",
    "pred_array = np.array([[label_dict.get(cat, 0) for cat in categories] for label_dict in pred_binary])\n",
    "\n",
    "# Evaluate using sklearn's classification report\n",
    "print(classification_report(true_array, pred_array, target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yigit's stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
