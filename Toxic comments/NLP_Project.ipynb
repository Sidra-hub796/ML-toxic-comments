{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 640.0 kB/s eta 0:00:20\n",
      "     ---------------------------------------- 0.1/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.6/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.9/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 8.1 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 7.1 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.3/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 7.9 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 8.4 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 9.0 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.6/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.1/12.8 MB 9.4 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.4/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.0/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 8.0/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.0/12.8 MB 10.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 11.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.1/12.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.4/12.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.0/12.8 MB 12.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.5/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 11.9 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Install spaCy (if not already installed)\n",
    "#!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas and scikit-learn for data handling and evaluation\n",
    "#!pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yigi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yigi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Yigi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yigi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install nltk\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"punkt\")  # Tokenization\n",
    "nltk.download(\"stopwords\")  # Stop words\n",
    "nltk.download(\"vader_lexicon\")  # Sentiment analysis\n",
    "nltk.download(\"wordnet\")  # Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "from spacy.pipeline.textcat import Config, single_label_cnn_config, single_label_bow_config, single_label_default_config\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train.csv\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the dataset:\n",
      "id               0\n",
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values and overall dataset structure\n",
    "print(\"Missing values in the dataset:\")\n",
    "print(train_data.isnull().sum())\n",
    "train_data.info()\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned comments:\n",
      "0    explanation edit make username hardcore metall...\n",
      "1    daww ! match background colour I m seemingly s...\n",
      "2    hey man I m really try edit war guy constantly...\n",
      "Name: comment_text, dtype: object\n",
      "Label distribution:\n",
      "toxic            15294\n",
      "severe_toxic      1595\n",
      "obscene           8449\n",
      "threat             478\n",
      "insult            7877\n",
      "identity_hate     1405\n",
      "dtype: int64\n",
      "Training samples: 127656, Validation samples: 31915\n",
      "Sample processed data (spaCy format):\n",
      "[('grandma terri burn trash grandma terri trash hate grandma terri fk hell ! 71747640', {'cats': {'toxic': True, 'severe_toxic': False, 'obscene': False, 'threat': False, 'insult': False, 'identity_hate': False}}), ('9 may 2009 utc would easiest admit member involve portuguese lodge would requirement acknowledge whether previous account carlos botelho good record would remove sockpuppet template irrelevant wpcoi permit people edit article msjapan mean careful ensure reference back edit npov uphold 2029', {'cats': {'toxic': False, 'severe_toxic': False, 'obscene': False, 'threat': False, 'insult': False, 'identity_hate': False}}), ('objectivity discussion doubtful nonexistent 1 indicate early section marxist leader view mislead lay unwarranted excessive emphasis trotsky create mislead impression prominent marxist marx engel lenin advocate andor practice terrorism b lay unwarranted excessive emphasis theoretical rejection individual terrorism create misleading impression main marxist position terrorism 2 discussion properly monitor discernible attempt make establish maintain acceptable degree objectivity b important relevant scholarly work international encyclopedia terrorism ignore illicitly exclude discussion c though logical way remedy blatant imbalance section include quote byon leader know endorse practiced terrorism attempt systematically block impunity apologist marxist terrorism do good sabotage wreck article discussion 3 among tactic deploy apologist wrecker saboteur follow may identify representative example claim marx engel advocate terrorism despite fact scholarly work like international encyclopedia terrorism show marx know red terror doctor b claim marx engel involve terrorist activity despite fact numerous source neue rheinische zeitung isaiah berlin francis wheen state otherwise c claim lenin refer terror proletarian revolution renegade k kautsky worksstatement despite fact robert service iet scholarly reliable source state claim russian word strakh mean terror oxford russian dictionary say ii evident context case ii educate russian speaker confirm strakh may mean terror depend context e claim marxism scientific fact marx scientist ii marxs background philosophy law science ii marxism recognize science academic world iv virtually every one marxs prediction turn wrong become increasingly apparent lifetime incontrovertibly death r pipe communism brief history 2001 p 15 follow marxism qualify scientific system accept standard v evidence indicate marxism close religious sect science proper f apologist literature quote fraudulent attempt whitewash marxist terrorism effect turn discussion advertisement terrorism g claim marxist terrorism root marxist theory class struggle even though numerous source show please note immaterial whether terrorism already justify term theory class prior marx point advocatedpractice basis marxist classtruggle theory marxist karl marx feel terror necessary part revolutionary strategy peter galvert theories terror urban insurrection iet p 138 revolutionary terrorism root political ideology marxistleninist thinking leave fascist find right noemi galor revolutionary terrorism iet p 203 perhaps important key stalin motivation lie realm ideology leitmotif soviet communist ideology 1920s 1930s class struggle inbuilt antagonism mutually incompatible economic interest group geoffrey robert stalins war 2006 pp 1718 fact support reliable academic source elementary logic 1907 mehring publish magazine neue zeit vol xxv 2 p 164 extract letter marx weydemeyer date march 5 1852 letter among thing follow noteworthy observation class struggle necessarily lead dictatorship proletariat', {'cats': {'toxic': False, 'severe_toxic': False, 'obscene': False, 'threat': False, 'insult': False, 'identity_hate': False}})]\n"
     ]
    }
   ],
   "source": [
    "#Disclaimerhiscelltakes22minutestorunonmymachine# Load stop words and spacy model\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text data.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove punctuation, but keep \"!\" and \"?\" for context\n",
    "    text = re.sub(r\"[^\\w\\s!?]\", \"\", text)\n",
    "    \n",
    "    # Normalize repeated characters (\"soooo\" -> \"soo\")\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    tokens = [token.lemma_ for token in nlp(\" \".join(tokens))]\n",
    "    \n",
    "    # Reconstruct text from processed tokens\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Function to transform the data into spacy format\n",
    "def prepare_spacy_data(comments, labels):\n",
    "    \"\"\"Transform text and labels into spacy-compatible format.\"\"\"\n",
    "    spacy_data = []\n",
    "    for comment, label in zip(comments, labels.values):\n",
    "        cats = {col: bool(value) for col, value in zip(labels.columns, label)}  # Convert labels to binary dict\n",
    "        spacy_data.append((comment, {\"cats\": cats}))\n",
    "    return spacy_data\n",
    "\n",
    "# Define inputs and outputs\n",
    "X = train_data[\"comment_text\"]\n",
    "y = train_data.drop(columns=[\"id\", \"comment_text\"])\n",
    "\n",
    "# Preprocess text data\n",
    "X_cleaned = X.apply(preprocess_text)\n",
    "\n",
    "# Print sample of preprocessed text\n",
    "print(\"Sample cleaned comments:\")\n",
    "print(X_cleaned[:3])\n",
    "\n",
    "# Inspect labels for imbalance\n",
    "print(\"Label distribution:\")\n",
    "print(y.sum(axis=0))\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_cleaned, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# Transform preprocessed data into spaCy format\n",
    "train_data_spacy = prepare_spacy_data(X_train, y_train)\n",
    "val_data_spacy = prepare_spacy_data(X_val, y_val)\n",
    "\n",
    "# Print sample of processed spaCy data\n",
    "print(\"Sample processed data (spaCy format):\")\n",
    "print(train_data_spacy[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'severe_toxic' class because of redunancy with toxic\n",
    "train_data = train_data.drop(columns=['severe_toxic'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input (comments) and output (labels)\n",
    "X = train_data['comment_text']\n",
    "y = train_data.drop(columns=['id', 'comment_text'])\n",
    "\n",
    "# Inspect labels for imbalance\n",
    "print(\"Label distribution:\")\n",
    "print(y.sum(axis=0))  # Check class counts per label\n",
    "\n",
    "# Split the data (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform the data into spaCy format\n",
    "def prepare_spacy_data(comments, labels):\n",
    "    spacy_data = []\n",
    "    for comment, label in zip(comments, labels.values):\n",
    "        # Create a dictionary of labels with their binary values\n",
    "        cats = {col: bool(value) for col, value in zip(labels.columns, label)}\n",
    "        spacy_data.append((comment, {'cats': cats}))\n",
    "    return spacy_data\n",
    "\n",
    "# Prepare training and validation data\n",
    "train_data_spacy = prepare_spacy_data(X_train, y_train)\n",
    "val_data_spacy = prepare_spacy_data(X_val, y_val)\n",
    "\n",
    "# Print a sample of the processed data\n",
    "print(\"Sample processed data (spaCy format):\")\n",
    "print(train_data_spacy[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespaces\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to training and validation data\n",
    "X_train_cleaned = X_train.apply(preprocess_text)\n",
    "X_val_cleaned = X_val.apply(preprocess_text)\n",
    "\n",
    "# Prepare spaCy data again with cleaned text\n",
    "train_data_spacy = prepare_spacy_data(X_train_cleaned, y_train)\n",
    "val_data_spacy = prepare_spacy_data(X_val_cleaned, y_val)\n",
    "\n",
    "print(\"Sample cleaned data (spaCy format):\")\n",
    "print(train_data_spacy[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to files (optional)\n",
    "# import json\n",
    "\n",
    "# with open(\"train_data_spacy.json\", \"w\") as f:\n",
    "#     json.dump(train_data_spacy, f)\n",
    "\n",
    "# with open(\"val_data_spacy.json\", \"w\") as f:\n",
    "#     json.dump(val_data_spacy, f)\n",
    "\n",
    "# print(\"Preprocessed data saved as JSON files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank spaCy pipeline for English\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Print a confirmation\n",
    "print(\"Blank spaCy pipeline created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"threshold\": 0.5,  # Classification threshold\n",
    "    \"model\": {\n",
    "        \"@architectures\": \"spacy.TextCatEnsemble.v2\",\n",
    "        \"tok2vec\": {\n",
    "            \"@architectures\": \"spacy.Tok2Vec.v2\",\n",
    "            \"embed\": {\n",
    "                \"@architectures\": \"spacy.MultiHashEmbed.v2\",\n",
    "                \"width\": 64,\n",
    "                \"rows\": [2000, 2000, 500, 1000, 500],\n",
    "                \"attrs\": [\"NORM\", \"LOWER\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"],\n",
    "                \"include_static_vectors\": False,\n",
    "            },\n",
    "            \"encode\": {\n",
    "                \"@architectures\": \"spacy.MaxoutWindowEncoder.v2\",\n",
    "                \"width\": 64,\n",
    "                \"window_size\": 1,\n",
    "                \"maxout_pieces\": 3,\n",
    "                \"depth\": 2,\n",
    "            },\n",
    "        },\n",
    "        \"linear_model\": {\n",
    "            \"@architectures\": \"spacy.TextCatBOW.v3\",\n",
    "            \"exclusive_classes\": False,  # Multi-label classification\n",
    "            \"ngram_size\": 1,\n",
    "            \"no_output_layer\": False,\n",
    "            \"length\": 262144,  # Add length explicitly to avoid further errors\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Add the multi-label text categorizer\n",
    "textcat = nlp.add_pipe(\"textcat_multilabel\", config=config)\n",
    "\n",
    "# Add labels (categories) to the text categorizer\n",
    "for label in y_train.columns:  # Assuming y_train.columns contains category names\n",
    "    textcat.add_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Training parameters\n",
    "n_iter = 1  # Number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_iter):\n",
    "    random.shuffle(train_data_spacy)  # Shuffle training data each epoch\n",
    "    losses = {}\n",
    "    \n",
    "    # Create batches of data\n",
    "    batches = minibatch(train_data_spacy, size=compounding(4.0, 32.0, 1.001))\n",
    "    \n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, annotations in batch:\n",
    "            # Create Example objects\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, {\"cats\": annotations[\"cats\"]})  # Multi-label format\n",
    "            examples.append(example)\n",
    "        \n",
    "        # Update the model with the batch of Example objects\n",
    "        nlp.update(examples, drop=0.5, losses=losses)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {losses['textcat_multilabel']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions and true labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for text, annotations in val_data_spacy:  # val_data is a list of (text, annotations)\n",
    "    # Convert the text and annotations into an Example\n",
    "    example = spacy.training.Example.from_dict(nlp.make_doc(text), {\"cats\": annotations[\"cats\"]})\n",
    "    \n",
    "    # Process the text with the model\n",
    "    doc = nlp(example.text)\n",
    "    \n",
    "    # Collect the predictions\n",
    "    pred_labels.append({label: doc.cats[label] for label in doc.cats})\n",
    "    \n",
    "    # Collect the true labels\n",
    "    true_labels.append(annotations[\"cats\"])\n",
    "\n",
    "# Convert predictions to binary based on threshold\n",
    "threshold = 0.5\n",
    "pred_binary = [\n",
    "    {label: int(score >= threshold) for label, score in pred.items()}\n",
    "    for pred in pred_labels\n",
    "]\n",
    "\n",
    "# Extract the keys (categories/labels) that are present in all true labels\n",
    "categories = list(set.intersection(*[set(label_dict.keys()) for label_dict in true_labels])) # Made categories only include labels present in all true_labels to avoid KeyError\n",
    "\n",
    "# Convert dictionaries to 2D arrays for sklearn\n",
    "true_array = np.array([[label_dict.get(cat, 0) for cat in categories] for label_dict in true_labels])\n",
    "pred_array = np.array([[label_dict.get(cat, 0) for cat in categories] for label_dict in pred_binary])\n",
    "\n",
    "# Evaluate using sklearn's classification report\n",
    "print(classification_report(true_array, pred_array, target_names=categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yigit's stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
